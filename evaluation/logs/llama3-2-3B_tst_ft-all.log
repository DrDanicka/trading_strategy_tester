Loading pretrained model

Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]
Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 32151.46it/s]
Loading datasets
Training
Trainable parameters: 50.135% (1610.711M/3212.750M)
Starting training..., iters: 100
Iter 1: Val loss 3.368, Val took 60.938s
Iter 10: Train loss 1.488, Learning Rate 1.000e-05, It/sec 0.056, Tokens/sec 76.439, Trained Tokens 13752, Peak mem 20.167 GB
Iter 20: Val loss 0.721, Val took 59.416s
Iter 20: Train loss 0.735, Learning Rate 1.000e-05, It/sec 0.062, Tokens/sec 78.405, Trained Tokens 26452, Peak mem 20.167 GB
Iter 30: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.057, Tokens/sec 78.261, Trained Tokens 40212, Peak mem 20.167 GB
Iter 40: Val loss 0.566, Val took 64.666s
Iter 40: Train loss 0.590, Learning Rate 1.000e-05, It/sec 0.055, Tokens/sec 81.504, Trained Tokens 54912, Peak mem 21.263 GB
Iter 50: Train loss 0.543, Learning Rate 1.000e-05, It/sec 0.056, Tokens/sec 72.595, Trained Tokens 67897, Peak mem 21.263 GB
Iter 60: Val loss 0.529, Val took 62.943s
Iter 60: Train loss 0.548, Learning Rate 1.000e-05, It/sec 0.051, Tokens/sec 79.862, Trained Tokens 83449, Peak mem 21.263 GB
Iter 70: Train loss 0.519, Learning Rate 1.000e-05, It/sec 0.058, Tokens/sec 80.019, Trained Tokens 97259, Peak mem 21.263 GB
Iter 80: Val loss 0.508, Val took 59.288s
Iter 80: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.058, Tokens/sec 77.417, Trained Tokens 110699, Peak mem 21.263 GB
Iter 90: Train loss 0.525, Learning Rate 1.000e-05, It/sec 0.060, Tokens/sec 76.672, Trained Tokens 123585, Peak mem 21.263 GB
Iter 100: Val loss 0.503, Val took 59.909s
Iter 100: Train loss 0.509, Learning Rate 1.000e-05, It/sec 0.047, Tokens/sec 65.587, Trained Tokens 137629, Peak mem 21.263 GB
Iter 100: Saved adapter weights to adapters/adapters_llama3-2-3B_tst_ft-all/adapters.safetensors and adapters/adapters_llama3-2-3B_tst_ft-all/0000100_adapters.safetensors.
Saved final weights to adapters/adapters_llama3-2-3B_tst_ft-all/adapters.safetensors.
